{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232deffa-802e-4288-951a-6f83f7ef36cb",
   "metadata": {},
   "source": [
    "#### The Layer\n",
    "\n",
    "A simple linear regression model has 2 inputs and a single output but has no depth. We use a liner model that learn the function which was the best fit of the data. Most real life dependencies cannot be modelled with a simple Linear combination. To be better forecasters, we need better models which are more sophisticated than a linear model. Such complexitities are achieved by using both linear and non-linear operations.\n",
    "\n",
    "Mixing linear and non-linearities allow us to model arbitrary functions with conventional shapes. our model changes from inputs that are linearly combined resulting in outputs to inputs that are linearly combined and then go through some non-linearity resulting in outputs.\n",
    "\n",
    "A commonly used non-linearity is the sigmoid function.\n",
    "\n",
    "#### Layer\n",
    "\n",
    "The initial linear combination and the added non-linearity form a layer. The layer is the building block of neural networks. When we have more than one layer, we are talking about a deep neural network.\n",
    "\n",
    "#### What is Deep Net?\n",
    "\n",
    "- Our first layer is called the INPUT LAYER which is basically the data we haced 3ve.\n",
    "- We take the inputs and get outputs. The main rational about neural network is that we can now use these outputs as inputs for another layer until we decide to stop. These are the layers in between called the HIDDEN LAYERS. They are called hidden because we know the inputs and outputs but the rest remains hidden.\n",
    "- The last layer is the OUTPUT LAYER which is basically what we compare the targets to.\n",
    "- Stacking layers\n",
    "\n",
    "Deep net explains what happens from the point data is inputed into the model until we get an output. It explains the stacking of layers. Stacking layers is the process of placing one layer after another \n",
    "\n",
    "#### Understanding Deep nets\n",
    "\n",
    "Input => Output\n",
    "\n",
    "First layer - Input layer - Data: Each cycle is a separate input and it is a place holder for the inputs. Each arrow represents the mathematical transformation of a given value.\n",
    "\n",
    "Last layer - Output layer (The last hidden unit)\n",
    "\n",
    "Layers in-between are called the hidden layers.\n",
    "\n",
    "When we feed the input layer(Data), it produces a hidden layer which becomes an input layer for the next hidden layer and so on. The number of hidden layers we have depends on how deep we want our network to be. \n",
    "\n",
    "The Hyperparameters are set by the data scientists. Here,\n",
    "\n",
    "- The width refers to the the nodes.\n",
    "- The depth refers to the number of hidden layers.\n",
    "\n",
    "Parameters are inbuilt in the machine. Here, we have:\n",
    "- The weights\n",
    "- The biases\n",
    "\n",
    "The difference between the two is that the value of parameters will be derived through optimization.\n",
    "\n",
    "##### Why do we need non-linearities?\n",
    "\n",
    "Non- linearities are needed so we can break the linearity and represent more complicated relationships. An important concept for non-linearity is the ability to stack layers. Stacking layers is the process of placing one layer after the other in a meaningful way. We cannot stack layers when we have only linear relationshi\n",
    "Two consecutive linear transformations are equivalent to a single one.\n",
    "\n",
    "Non-linearities -> Stacking layers -> Depth -> Deep learning\n",
    "\n",
    "Summary: In order to have deep nets and find complex relationships through arbitrary functions, we need non-linearities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51fbc5-0f15-44ee-af51-e5aa3ce44d41",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "\n",
    "In ML context, non-linearities are also called activation functions.\n",
    "\n",
    "They are also called transfer functions because of their transformation properties.\n",
    "\n",
    "Activation functions transform inputs into outputs of a different kind.\n",
    "\n",
    "In ML, there are different acivation functions\n",
    "1. Sigmoid(logistic function) - once we apply the sigmoid as an activator, all inputs will be in the range (0, 1).\n",
    "2. TanH(hyperbolic tangent)\n",
    "3. ReLu(rectified linear unit)\n",
    "4. Softmax- has no definite graph\n",
    "\n",
    "All common activation function are: Monotonic, Continuous and Differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b27fa-5c57-4668-ac7a-221715baf026",
   "metadata": {},
   "source": [
    "#### Softmax Activation\n",
    "\n",
    "The softmax takes as argument the whole vector a. While the order activation functions get an input value and transform it regardless of the order arguments, the softmax considers the information from all elements.\n",
    "\n",
    "Softmax is special. Each element in the output depends on the entire set of elements of the input. \n",
    "\n",
    "The softmax transformation transforms a bunch or arbitrarily large or small numbers into a valid probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a595eff-ebbe-4ab2-9f7f-03d5acad704e",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "Forward propagation is the process of pushing inputs through the net. At the end of each epoch, we backpropagate and change each parameter accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ecca3-6da6-46fe-8f0b-f41d3187284f",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "This is the process in which we set the initial values of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739c261-a8e1-43fe-aaec-084030f8c7c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Overfitting and Underfitting\n",
    "\n",
    "##### Overfitting: \n",
    "\n",
    "It means our training has focused on the particular training set so much, it has missed the point. \n",
    "\n",
    "Overfitted model is so super good at modelling the training data that it misses the point. It would model/capture all the noise. It also underperforms greatly.\n",
    "\n",
    "Low loss and low accuracy.\n",
    "\n",
    "##### Underfitting: \n",
    "\n",
    "This means that the model has not captured the underlying logic of the data. \n",
    "\n",
    "An underfitted model will provide an answer but does not capture the underlying logic.\n",
    "\n",
    "High loss and low accuracy are indicators of a bad model.\n",
    "\n",
    "An underfitted model is a linear model.\n",
    "\n",
    "\n",
    "Bias Variance: It is the balance between underfitting and overfitting\n",
    "\n",
    "\n",
    "##### Dealing with Overfitting\n",
    "\n",
    "- Identify it first by dividing our dataset set into training, validation and test.\n",
    "    . Training: this is where the training happens.\n",
    "    . Validation: helps us prevent overfitting.We run the model on the validation dataset\n",
    "    . Testing: this measures the final predictive power of the model. This is done by running the model on a new dataset it has never seen before.\n",
    "\n",
    "- At the point where training loss is decreasing and the validation loss is increasing, we are overfitting. But when training and validation loss go hand in hand, proceed.\n",
    "- The accuracy of the test dataset is what we expect the model to have if deployed in real life.\n",
    "- The common practise is to split the dataset into Training(80% or 70%), Validation (10% or 20%) and Testing(10%). \n",
    "\n",
    "Summary:\n",
    "\n",
    "1. Get a dataset. \n",
    "2. Split into 3 parts - training, validation and test dataset.\n",
    "3. Train the model using the training set only.\n",
    "4. Validate the model every now and then by running it from the validation dataset.\n",
    "5. Test the model with the test dataset. The accuracy obtained at this stage is the accuracy of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ce2ff-36ed-434f-8647-4df65a58c8b9",
   "metadata": {},
   "source": [
    "#### N-Fold Cross validation\n",
    "\n",
    "When we have a small dataset, we cant afford to split into 3 because the algorithm may not learn anything.\n",
    "\n",
    "N-Fold Cross-validation - a strategy that combines the training and validation dataset in a clever way. The test stage cannot be avoided though.\n",
    "\n",
    "Pros: It utilizes more data and we have a model.\n",
    "\n",
    "Cons: Possibilty of a slight overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df435b0-c063-487f-981b-a80e770e4e30",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "\n",
    "This is a technique to prevent overfitting. We stop early before we overfit.\n",
    "\n",
    "Ways to do this:\n",
    "1. Train for a preset number of epochs. \n",
    "    \n",
    "    Pros: Eventually solves the problem.\n",
    "    \n",
    "    Cons: No guarantee that the min is reached.\n",
    "          Maybe it doesnt minimize at all\n",
    "          Use the Naive method.\n",
    "    \n",
    "2. Stop when updates become too small.\n",
    "    \n",
    "    Pros:  We are sure the loss is minimized\n",
    "    \n",
    "           Saves computing power\n",
    "           \n",
    "     Cons: Can lead to overfitting\n",
    "    \n",
    "3. Validation set strategy.\n",
    "    \n",
    "    Pros: We are sure the validation loss is minimized\n",
    "          \n",
    "          Saves computing power.\n",
    "              \n",
    "    Cons: It may take our algorithm a very long time to overfit.\n",
    " \n",
    "Best practise is to stop when the validation loss starts increasing or when the training loss becomes very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a177f9f-51ca-44ee-bd32-9660a3d300ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
