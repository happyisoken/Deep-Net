{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b985c670-9c26-4c81-b4dc-f820885ea2d2",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "This refers to the algorithms we will use to vary our model's parameters.\n",
    "\n",
    "Steps to take to train our model:\n",
    "1. Gradient descent(GD) - It iterates over the whole training set before updating the weights. It is slow and not descending.\n",
    "2. Stochastic Gradient descent(SGD) - It is similar to GD and works in the same way but updates the weights many times inside a single epoch. It is much faster. The SGD comes at a cost though - it approximates things a bit.\n",
    "\n",
    "##### Gradient Descent Pitfalls(Local Minima Pitfalls)\n",
    "\n",
    "A single GD wold be slow, but will eventually reach the minimum.\n",
    "\n",
    "SGD would be much faster, but will give us an approximate answer.\n",
    "\n",
    "Each local minimum is a suboptimal solution to the optimization problem. GD is prone to this issue.It often falls to the closest minimum to the starting point rather than a global minimum.\n",
    "\n",
    "#### Momemtum\n",
    "\n",
    "Both GD and SGD are good ways to train our models. We need not change them but extend them. The simplest extention to apply is momentum.\n",
    "\n",
    "Momentum: We create algorithms that will likely fall into a depth instead of descending to the optimal solution. Including momentum, we ill consider the speed with which we are descending.\n",
    "\n",
    "#### Learning Rate Schedules\n",
    "\n",
    "Hyperparemeters are set by us - They are the width, depth and learning rate.\n",
    "\n",
    "Parameters are found by optimizing - They are the weights and biases.\n",
    "\n",
    "The Learning rate iter - it must be small enough so we gently descend, instead of oscillating widely around the minimum or diverging to infinity. It also has to be big enough so the optimiztion takes place in a reasonable amount of time.\n",
    "\n",
    "A smart way is deal with choosing the proper learning rate is to adopt a learning rate schedule.\n",
    "\n",
    "Learning rate schedule deals both the small enough and big enough.\n",
    "\n",
    "Small enough \n",
    "1. Start from a high initial learning rate - leads to faster training\n",
    "2. At some point, we lower the rate to avoid oscillation.\n",
    "3. Around the end, we pick a very small rate to get a precise answer.\n",
    "\n",
    "How learning schedules are implemented in practise:\n",
    "- The simplest way is to set a pre-determined piecewise learning rate.\n",
    "- Exponential schedule - still simple but much better as it smoothly decays the learning rate.\n",
    "\n",
    "#### Advanced Learning Rate Schedules\n",
    "\n",
    "Types:\n",
    "1. AdaGrad (Adaptive Gradient Algorithm): It dynamically varies the learning rate at each update and for each weight individually. Just ask TesorFlow to use AdaGrad please.\n",
    "    - it is smart\n",
    "    - it makes use of Adaptive learning rate schedule\n",
    "    - it is based on the training itself\n",
    "    - the adaption is Per weight\n",
    "\n",
    "2. RMSProp (The Root Mean Square Propagation): Similar to AdaGrad. It is longer monotonous, so it can adapt upwards and downwards. Both methods are logical and smart.\n",
    "\n",
    "3. Adam (Adaptive moment estimation): It is the most advance optizer(very fast and efficient). It includes momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed0dcf-f2b0-41ed-b6e4-13eaeef51125",
   "metadata": {},
   "source": [
    "#### Preprocessing \n",
    "\n",
    "This is the first activity before creating a ML Algorithm. It refers to any manipulation applied to a dataset before running it through the model.\n",
    "\n",
    "The motivation for preprocessing:\n",
    "1. Compatibility with the library used.\n",
    "2. Orders of magnitude - adjusting the input of different magnitude.\n",
    "3. Generalization - same model, differen issue.\n",
    "\n",
    "#### Basic Preprocessing \n",
    "\n",
    "Relative metrics are especially useful when we have a time-series data.\n",
    "\n",
    "The realtives can further be transformed into Logarithms.\n",
    "\n",
    "Advantages:\n",
    "- Faster computation\n",
    "- Lower order of magnitude.\n",
    "- Clearer relationships\n",
    "- Homogeneous variance\n",
    "\n",
    "#### ML Preprocessing - Standardization\n",
    "\n",
    "Also known as Feature Scaling and Normalization.\n",
    "\n",
    "Standardization/Feature Scaling is the process of transforming data into a standard scale. This is done by subtracting the mean from the original variable and dividing by the standard deviation.\n",
    "\n",
    "Other methods include:\n",
    "- Normalization - It consists of converting each sample into a unit length vector using the L1 or L2 Norm.\n",
    "- PCA(Principal Components analysis) - Adimension reduction technique used to combine several variables into a bigger variable. \n",
    "- Whittening: Often performed after PCA. It removes most underlying correlations between data points. \n",
    "\n",
    "#### Dealing with Categorical Data\n",
    "\n",
    "Categorical Data refers to groups or categories (non-numerical data). The ML algorithm takes only numbers as values. Transforming categorical data will mean to assign numerical values to each category/group. \n",
    "\n",
    "The 2 main ways to encode categories in a way useful for ML are:\n",
    "1. One-hot encoding\n",
    "2. Binary encoding\n",
    "\n",
    "#### One-Hot vs Binary Encoding\n",
    "\n",
    "- Binary encoding implies turning ordinal numbers into binary i.e 0 and 1. \n",
    "\n",
    "- One-Hot encoding consists of creating as many columns as there are possible values. \n",
    "\n",
    "The trade-off between binary and one-hot encoding:\n",
    "- We use one-hot encoding when there are few categories.\n",
    "- We use binary when there are many categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d612c-8e00-4ea9-be2c-3aaf1b13f44c",
   "metadata": {},
   "source": [
    "#### MNIST Classification\n",
    "\n",
    "The MNIST dataset consists of 70,000 images of handwritten digits.It is the 'Hello World' of ML. There are 10 classes from 0 - 9. The objective is to build an algorithm that takes aa input an image and then correctly determines which number is shown in that image.\n",
    "\n",
    "Reasons for this Algorithm\n",
    "1. It is a visual problem - you can see the data and know what to expect.\n",
    "2. It is extremely common.\n",
    "3. It is easy to build up the CNN(Convolusional Neural Networks) from the MNIST example.\n",
    "4. Very big and processed dataset - i.e. the dataset is large and clean; no missing values, wrong labels etc\n",
    "\n",
    "Read more on yann.lecun.com.\n",
    "\n",
    "#### How to tackle Image Recognition problem\n",
    "\n",
    "We can think about the problem as a 28x28 matrix, where input values are from 0 to 255. 0 -> black, 255 -> white. A 28x28 photo will have 784 pixels.\n",
    "\n",
    "The approach for deep neural networks is to 'flatten' each image into a vector 784 x 1.\n",
    " - Each photo consists of 784 pixels.\n",
    " - Each pixel is an input for our neural network.\n",
    " - Each pixel corresponds to the intensity of the color (255 to white, 0 is black)\n",
    "\n",
    "##### The MNIST deep net\n",
    "\n",
    "Each pixel is an input in the input layer. We will linearly combine them and add a non-linearity to get the first hidden layer. For our example, we will build a model with 2 hidden layers. We then produce the output layer. There are 10 digits => 10 classes => Therefore, 10 output units in the output layer. The output will then be compared to the target. Using a softmax activation function for the output layer.\n",
    "\n",
    "##### The MNIST action plan\n",
    "1. Prepare our data and preprocess it. Create a  training, validation and test dataset.\n",
    "2. Outline the model and choose the activation functions we want to employ.\n",
    "3. Set the appropriate advanced optimizer and the loss function.\n",
    "4. Make the data learn.\n",
    "5. Test the accuracy of the model regarding the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e2cb9-c21a-42ef-93c8-d89b80f0e83f",
   "metadata": {},
   "source": [
    "#### Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll aply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the 'Hello World' of deep learning because for most students, it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as Convolutional Neural Networks (CNNs).\n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc5760-0ce3-4f3d-85d7-fe95495f5978",
   "metadata": {},
   "source": [
    "#### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68703559-100d-4645-ad5d-6231e9edbe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04b2095-5451-4ce7-88f6-e9aff8f481d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08466c-4fc7-4904-9c8e-91b41d99ca7a",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da86316-662d-45ae-9aea-28c86710a049",
   "metadata": {},
   "source": [
    "mnist_dataset = tfds.load(name = 'mnist')\n",
    "#tfds has a large number of datasets ready for modelling\n",
    "#the first time you execute tfds.load(), a dataset willbe downloaded on your computer.\n",
    "#Each consecutive time you run the code, it will automatically load this local copy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b919f-8262-4c37-972e-2408c1f3c8a4",
   "metadata": {},
   "source": [
    "mnist_datset, mnist_info = tfds.load(name = 'mnist', with_info = True, as_supervised=True)\n",
    "\n",
    "#tfds.load(name, with_info, as_supervised) loads a dataset from TensorFlow datasets.\n",
    "#as_supervised = True, loads the data in a 2-tuple structure [input, target].\n",
    "#with_info = true, provides a tuple containing info about version, features, samples of the dataset\n",
    "#the first time it will take a bit longer(since you're actually downloading the dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18659710-327a-4682-92ac-e7abd51d11ac",
   "metadata": {},
   "source": [
    "# tfds.loads(name,with_info,as_supervized) :- \n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated \n",
    "# (1) as supervised - True,loads the data in a 2-tuple structure(inputs,targets)\n",
    "# (2) with_info - True,provides a tuple containing information about version,features, #samples of the d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba2bcb3-8ed0-4b52-a87a-711e44402528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "#Acquiring our data and storing in the mnist dataset.\n",
    "mnist_dataset,mnist_info = tfds.load('mnist',with_info = True, as_supervised = True)\n",
    "\n",
    "#tfds has a large number of datasets ready for modelling\n",
    "#the first time you execute tfds.load(), a dataset willbe downloaded on your computer.\n",
    "#Each consecutive time you run the code, it will automatically load this local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec69755f-50b1-47ea-a922-578634a8a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the train and test data\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "#Let's take 10% of the training dataset to serve as validation. \n",
    "#We can either count the num of train samples, or use the mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdce645b-7210-4386-802a-fd453da8380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The validation dataset\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "#tf.cast(x, dtype) casts (converts) a variable into a given data type\n",
    "\n",
    "#store the number of test samples in a dedicated variable\n",
    "num_test_samples = mnist_info.splits['train'].num_examples\n",
    "num_test_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "#normally, we'd like to scale our data in some way to make the result more numerically stable(e.g. inputs bet 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ab6d4-68cd-4acd-adf0-ed6d90424a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will scale the inputs called scale\n",
    "def scale(image, label):\n",
    "# we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "# since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "# if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "#dataset.map(*function*) applies a custom transformation to a given dataset.\n",
    "#It takes as input a function which determines the transformation to a given dataset\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "#this will scale the whole train dataset and store it in our new variable\n",
    "\n",
    "test_dataset = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000 #this is used in case we are dealing with enormous dataset\n",
    "\n",
    "#when we are dealing with enormous datasets, we can't shuffle all data at once\n",
    "#if buffer_size = 1, no shuffling will happen\n",
    "#if buffer_size .= num_samples, shuffling will happen at once(uniformly)\n",
    "#if 1 < buffer_size < num_samples, we will be optimizing the computational power\n",
    "\n",
    "#we apply shuffling in the preprocessing stage.\n",
    "#Shuffling = Keeping the same information but in a different order.\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "#extracting the train and validation dataset\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "#set a batch size and prepare our data for batching\n",
    "#batch size of 1 = Stochastic gradient descent(SGD)\n",
    "#batch size = num of samples = (single batch) GD\n",
    "#1 < batch size < num of samples = mini-batch GD\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#dataset.batch(batch_size) a method that combines the consecutive elements of a dataset into batches\n",
    "train_data = train_data.batch(BATCH_SIZE) #this adds a column to our tensor indicating num of sample in each batch\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "#test data = test_data(num_test_samples)???\n",
    "                      \n",
    "#when batching, we find the average loss and average accuracy\n",
    "#the model expects the validation dataset in batch form too.\n",
    "\n",
    "#extract and convert the validation inputs and target appropriately\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505dab3-da98-4c78-a120-390a114bf4d7",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "##### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df904427-d9cd-4fa9-b56f-715e000f35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare 3 variables for width of the input, output and hidden layer\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "#the underlying assumption is that all hidden layers are of the same size\n",
    "\n",
    "#define the actual model and store in a variable called model\n",
    "#our data(from tfds) is such that each input is 28x28x1\n",
    "#tf.keras.layers.Flatten(original shape) transforms (flattens) a tensor into a vector\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c7e79-7235-4451-bdc2-7516c30f977c",
   "metadata": {},
   "source": [
    "tf.keras.layers.Dense(output size) takes the inputs, provided to the model and calculates the dot product of the inputs and the weights and adds the bias. This is also where we can apply an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011be5ce-19ed-409d-9a40-77c9b84c8c00",
   "metadata": {},
   "source": [
    "#### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f11d30e-f011-4734-9fcc-db73cbe54fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the optimizer and the loss through the compile method\n",
    "model.compile(optimizer ='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "# Adam - Adaptive Moment Estimation\n",
    "#configures the model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74933c20-a07e-44d1-a580-4969c8dd22ff",
   "metadata": {},
   "source": [
    "Types of Crossentropy:\n",
    "1. Binary Crossentropy - used when we've got binary encoding\n",
    "2. Categorical Crossentropy - expects that you've one-hot encoded the targets\n",
    "3. Sparse Categorical Crossentropy - applies one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330b2af-2df8-4947-84a2-02027aae67ad",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ff0c9f-fc40-435b-b66c-cfaf7dae8d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 10s - loss: 0.3980 - accuracy: 0.8877 - val_loss: 0.2292 - val_accuracy: 0.9403 - 10s/epoch - 19ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 7s - loss: 0.1810 - accuracy: 0.9475 - val_loss: 0.1708 - val_accuracy: 0.9525 - 7s/epoch - 12ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 8s - loss: 0.1395 - accuracy: 0.9589 - val_loss: 0.1557 - val_accuracy: 0.9570 - 8s/epoch - 14ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 7s - loss: 0.1136 - accuracy: 0.9663 - val_loss: 0.1284 - val_accuracy: 0.9662 - 7s/epoch - 13ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 7s - loss: 0.0974 - accuracy: 0.9703 - val_loss: 0.1101 - val_accuracy: 0.9680 - 7s/epoch - 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182e4bee860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model we have built\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195f4b2-50c2-4afc-9fef-e156e9aab80a",
   "metadata": {},
   "source": [
    "WHAT HAPPENS INSIDE AN EPOCH\n",
    "1. At the beginning of each epoch, the training loss will be set to 0\n",
    "2. The algorithm will iterate over a preset number of batches, all from tran_data.\n",
    "3. The weights and biases will be updated as many times as there are batches.\n",
    "4. We will get a value for the loss function, indicating how the training is going.\n",
    "5. We will also see a training accuracy.\n",
    "6. At the end of the epoch, the algorithm will forward propagate the whole validation set.\n",
    "*When we reach the maximum number of epochs, the training will be over."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1f531-99e1-4d01-a0ab-f4cd969be196",
   "metadata": {},
   "source": [
    "Explaining the result:\n",
    "- Info on the number of Epochs\n",
    "- Number of batches = 540/540\n",
    "- The time it took for the epoch to conclude\n",
    "- The training loss\n",
    "- The accuracy - it shows in what % of the cases our outputs were equal to the targets.\n",
    "- The loss and the accuracy for the validation dataset. val_accuracy = true accuracy of the model.\n",
    "- To access the overall accuracy of the model, check the val_accuracy for the last epoch (97%) - this is the validation accuracy.\n",
    "- val_accuracy = True Accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da37f54-1752-4e5c-96bc-2710c90e1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's increase the hidden layer to 100 and re-run\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbaca9a7-2103-4087-8129-9aadb47904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the optimizer and the loss through the compile method\n",
    "model.compile(optimizer ='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83a9cc9-b771-4162-886f-e82a3ee9dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 9s - loss: 0.3359 - accuracy: 0.9050 - val_loss: 0.1946 - val_accuracy: 0.9497 - 9s/epoch - 17ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 7s - loss: 0.1386 - accuracy: 0.9591 - val_loss: 0.1303 - val_accuracy: 0.9665 - 7s/epoch - 13ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 7s - loss: 0.0982 - accuracy: 0.9705 - val_loss: 0.1036 - val_accuracy: 0.9717 - 7s/epoch - 13ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 0.0753 - accuracy: 0.9774 - val_loss: 0.0919 - val_accuracy: 0.9750 - 6s/epoch - 12ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 7s - loss: 0.0604 - accuracy: 0.9813 - val_loss: 0.0769 - val_accuracy: 0.9790 - 7s/epoch - 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182e5f68b50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe4f51-e376-4b95-b5e1-e1d66b56cda0",
   "metadata": {},
   "source": [
    "With an increased hidden layer, the val_accuracy increased to 97.90%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94459f4d-c2ad-459b-806d-0049c8eb223d",
   "metadata": {},
   "source": [
    "#### Test the model\n",
    "\n",
    "We train on the training data and validate on the validation data. We must make sure our parameters - the weights and the biases - dont overfit.\n",
    "\n",
    "Create a model -> fiddle with the hyperparameters -> check validation accuracy. \n",
    "\n",
    "What we find are the hypeparmeters that fit our validation data best. By fine tuning them, we are overfitting the validation dataset.\n",
    "\n",
    "The validation dataset is our reality check that prevents us from overfitting the parameters. The validation accuracy is a benchmark for how good the model is.\n",
    "\n",
    "The test dataset is our reality check that prevents us from overfitting the hyperparameters(make sure our hyperparameters - width, depth, batch size, epochs etc. - don't overfit). It is the dataset the model has truly never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac125df7-1874-4f6f-a632-6ab6cff5b51f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#we can access the test accuracy by using the method evaluate.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mtest_data\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "#we can access the test accuracy by using the method evaluate.\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b271ce-6e9c-406a-801c-de7521383457",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{0:.2f}\u001b[39;00m\u001b[38;5;124m. Test accuracy: \u001b[39m\u001b[38;5;132;01m{1:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39m \u001b[38;5;28mformat\u001b[39m(\u001b[43mtest_loss\u001b[49m, test_accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100.\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'. format(test_loss, test_accuracy * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7586403-2d0c-4d86-a453-aec64313dd69",
   "metadata": {},
   "source": [
    "Our model has a test accuracy of 97.28%. This is the final state of the ML process. After testing the model, we are no longer allowed to change it.\n",
    "\n",
    "The main point of the test dataset is to simulate model deployment. If we get 50 to 60% testing accuracy, we will know that our model has overfit and it will fail miserably in real life. However, getting a test accuracy very close to the validation accuracy shows that we have not overfit.\n",
    "\n",
    "The test accuracy is the accuracy we expect to observe if we deploy the model in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a4e70-492d-485f-8c62-e02696a61f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
